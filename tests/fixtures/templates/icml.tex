\documentclass{article}
\IfFileExists{icml2024.sty}{\usepackage[accepted]{icml2024}}{\def\tylaxNoStyle{1}}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{hyperref}
\usepackage{natbib}
\ifdefined\tylaxNoStyle
\def\And{\\}
\fi
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{axiom}[theorem]{Axiom}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\title{Reinforcement Learning from Human Feedback at Scale}
\author{
Andrew Ng \\
Department of Computer Science \\
Stanford University \\
Stanford, CA, USA \\
DeepLearning.AI \\
\texttt{ang@stanford.edu}
\And
Fei-Fei Li \\
Department of Computer Science \\
Stanford University \\
Stanford, CA, USA \\
\texttt{feifeili@stanford.edu}
\And
Pieter Abbeel \\
EECS Department \\
UC Berkeley \\
Berkeley, CA, USA \\
Covariant \\
Emeryville, CA, USA \\
\texttt{pabbeel@cs.berkeley.edu}
}
\begin{document}
\maketitle
\begin{abstract}
 Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful paradigm for aligning large language models with human preferences. However, scaling RLHF presents significant challenges: reward model overfitting, distribution shift during policy optimization, and the need for extensive human annotation. We introduce Scalable RLHF (S-RLHF), a framework that addresses these challenges through three key innovations: (1) a robust reward model ensemble that mitigates overfitting, (2) constrained policy optimization with KL-divergence penalties, and (3) active learning strategies that minimize annotation requirements by 5x. Experiments on instruction following, summarization, and dialogue demonstrate that S-RLHF achieves 15\% higher human preference ratings while reducing compute requirements by 40\%. 
\end{abstract}
\section{Introduction}

Large language models (LLMs) pretrained on internet-scale data exhibit impressive capabilities but often generate outputs misaligned with human values \citep{ouyang2022training}. Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep} has emerged as the dominant approach for aligning these models, underlying systems like ChatGPT and Claude.

The standard RLHF pipeline consists of three stages:

\begin{enumerate}
  \item \textbf{Supervised Fine-Tuning (SFT)}: Fine-tune the pretrained model on demonstration data
  \item \textbf{Reward Modeling}: Train a reward model on human preference comparisons
  \item \textbf{Policy Optimization}: Use RL (typically PPO) to optimize the policy against the reward model
\end{enumerate}

Despite its success, scaling RLHF faces fundamental challenges:

\textbf{Reward Hacking.} The policy learns to exploit reward model weaknesses rather than genuinely improving quality \citep{gao2023scaling}. As optimization pressure increases, outputs become increasingly pathological.

\textbf{Distribution Shift.} The policy optimization changes the output distribution, but the reward model was trained on the SFT distribution \citep{kumar2022fine}. This leads to unreliable reward estimates.

\textbf{Annotation Cost.} Human preference collection requires O(100k) comparisons for competitive models \citep{bai2022training}, limiting iteration speed and model customization.

\section{Background}

\subsection{Reinforcement Learning from Human Feedback}

Given a prompt $x$ and response $y$, we model human preferences using a Bradley-Terry model: $P(y_1 succ y_2 | x) = \sigma(r_\phi (x, y_1) - r_\phi (x, y_2))$ where $r_\phi$ is the learned reward model and $\sigma$ is the sigmoid function.

The reward model is trained to minimize: $\mathcal{L}_\text{RM} = -\mathbb{E}_{(x, y_w, y_l) tilde \mathcal{D}} [log \sigma(r_\phi (x, y_w) - r_\phi (x, y_l))]$ where $y_w$ and $y_l$ are the preferred and dispreferred responses.

\subsection{Policy Optimization}

PPO \citep{schulman2017ppo} optimizes the policy $\pi_\theta$ to maximize expected reward while staying close to the reference policy $\pi_\text{ref}$: $\mathcal{L}_\text{PPO} = \mathbb{E}_{x tilde \mathcal{D}, y tilde \pi_\theta} [r_\phi (x, y) - \beta \text{KL}(\pi_\theta || \pi_\text{ref})]$

The KL penalty prevents the policy from deviating too far from the reference, which would lead to reward hacking.

\section{Method}

\subsection{Reward Model Ensembles}

We train an ensemble of $K$ reward models ${ r_\phi^{(k)} }_{k=1}^K$ with different random seeds. The final reward is: $r_\text{ens}(x, y) = (1)/K \sum_{k=1}^K r_\phi^{(k)}(x, y)$

\begin{theorem}
Let $\epsilon$ be the generalization error of individual reward models. An ensemble of $K$ independent models has expected error $\epsilon / \sqrt{K}$ under standard statistical assumptions.
\end{theorem}

\subsection{Constrained Policy Optimization}

We formulate RLHF as constrained optimization:

\begin{align*}
\max_{\theta} & \mathbb{E}_{x, y tilde \pi_\theta} [r_\text{ens}(x, y)] \
\text{s.t.} & \text{KL}(\pi_\theta || \pi_\text{ref}) \le \delta
\end{align*}

We solve this using the method of Lagrange multipliers, automatically tuning $\beta$ to satisfy the constraint.

\subsection{Active Learning for Preference Collection}

Not all preference comparisons are equally informative. We prioritize annotation of samples where:

\begin{enumerate}
  \item Reward model predictions are uncertain (high ensemble variance)
  \item The policy is likely to generate similar outputs (high density)
  \item Samples differ significantly from existing training data (diversity)
\end{enumerate}

Our acquisition function balances these factors: $a(x) = \sigma_\text{ens}(x) dot \sqrt{p_{\pi_\theta}(x)} dot \min_{(x', y') \in \mathcal{D}} d(x, x')$

\section{Experiments}

\subsection{Setup}

We evaluate on three tasks:

\begin{table}[t]
\centering
\caption{Evaluation setup across three alignment tasks.}
\begingroup
\begin{tabular}{cccc}
\textbf{\textbf{Task}} & \textbf{\textbf{Dataset}} & \textbf{\textbf{Metric}} & \textbf{\textbf{Samples}} \\
Instruction Following & FLAN + Self-Instruct & Win Rate & 50k \\
Summarization & TL;DR & Human Pref. & 100k \\
Dialogue & Anthropic HH & Helpfulness & 150k \\
\end{tabular}
\endgroup
\end{table}

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Win rate (\%) against human baseline across tasks. Higher is better.}
\begingroup
\begin{tabular}{cccc}
\textbf{\textbf{Method}} & \textbf{\textbf{Instruct}} & \textbf{\textbf{Summary}} & \textbf{\textbf{Dialogue}} \\
SFT Baseline & 45.2 & 48.1 & 44.7 \\
Standard RLHF & 58.3 & 61.2 & 56.8 \\
RLHF + Ensemble & 62.1 & 64.5 & 60.3 \\
S-RLHF (Ours) & \textbf{67.4} & \textbf{68.9} & \textbf{65.1} \\
\end{tabular}
\endgroup
\end{table}

S-RLHF consistently outperforms baselines across all tasks.

\subsection{Ablation Study}

\begin{table}[t]
\centering
\caption{Ablation study on instruction following task.}
\begingroup
\begin{tabular}{ccc}
\textbf{\textbf{Component}} & \textbf{\textbf{Win Rate}} & \textbf{$\Delta$} \\
S-RLHF (full) & 67.4 & -- \\
w/o ensemble & 62.1 & -5.3 \\
w/o constrained opt. & 64.8 & -2.6 \\
w/o active learning & 65.2 & -2.2 \\
Standard RLHF & 58.3 & -9.1 \\
\end{tabular}
\endgroup
\end{table}

\subsection{Annotation Efficiency}

Our active learning strategy achieves the same performance with 5x fewer annotations:

\begin{table}[t]
\centering
\caption{Win rate vs. number of preference annotations.}
\begingroup
\begin{tabular}{ccc}
\textbf{\textbf{Annotations}} & \textbf{\textbf{Random}} & \textbf{\textbf{Active}} \\
10k & 52.1 & 58.9 \\
25k & 56.4 & 64.2 \\
50k & 58.3 & 67.4 \\
100k & 60.1 & 67.8 \\
\end{tabular}
\endgroup
\end{table}

\section{Related Work}

\textbf{RLHF.} \citep{ouyang2022training} established RLHF for instruction following. \citep{bai2022training} scaled to Constitutional AI. \citep{rafailov2023dpo} proposed Direct Preference Optimization, avoiding explicit reward modeling.

\textbf{Reward Model Robustness.} \citep{gao2023scaling} studied reward model overoptimization. \citep{coste2023reward} proposed reward model ensembles. Our work extends ensembles with constrained optimization.

\textbf{Active Learning.} \citep{settles2009active} surveys active learning for classification. We adapt these ideas to preference learning with novel acquisition functions.

\section{Conclusion}

We presented S-RLHF, a scalable framework for alignment that addresses reward hacking, distribution shift, and annotation efficiency. Our experiments demonstrate consistent improvements across instruction following, summarization, and dialogue tasks. Future work includes scaling to larger models and extending to multi-turn interactions.

\section*{Acknowledgments}

We thank the annotation team for their careful work on preference labeling.

\bibliographystyle{plainnat}
\bibliography{refs}
\end{document}
